<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Modular Perl in Red Hat Enterprise Linux 8</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/03/10/modular-perl-red-hat-enterprise-linux-8" /><author><name>Petr Pisar</name></author><id>67e169d8-c02b-42b1-9ca0-803a73930efa</id><updated>2022-03-10T07:00:00Z</updated><published>2022-03-10T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; in &lt;a href="https://www.redhat.com/en/enterprise-linux-8"&gt;version 8&lt;/a&gt; (RHEL 8) comes with &lt;em&gt;modules&lt;/em&gt;, a packaging concept that allows system administrators to select the desired software version from multiple packaged versions. This article shows you how to manage Perl as a module, as well as how to manage the CPAN modules provided by Perl, in RHEL 8.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The term &lt;em&gt;module&lt;/em&gt; in this article is used for both RHEL modules and Perl modules. I will refer to the "modularity module" for the Red Hat Enterprise Linux type and the "CPAN module" for the Perl type.&lt;/p&gt; &lt;h2&gt;Installing Perl from the default stream&lt;/h2&gt; &lt;p&gt;Start by installing Perl in a simple manner:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;# yum --allowerasing install perl Last metadata expiration check: 1:37:36 ago on Tue 07 May 2019 04:18:01 PM CEST. Dependencies resolved. ========================================================================================== Package Arch Version Repository Size ========================================================================================== Installing: perl x86_64 4:5.26.3-416.el8 rhel-8.0.z-appstream 72 k Installing dependencies: […] Transaction Summary ========================================================================================== Install 147 Packages Total download size: 21 M Installed size: 59 M Is this ok [y/N]: y […] perl-threads-shared-1.58-2.el8.x86_64 Complete!&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, check which version of Perl you have:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ perl -V:version version='5.26.3';&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output shows that you have Perl 5.26.3. This is the default version supported for the next 10 years. If you are fine with that, then you don't need to worry about modules. But what if you want to try a different version? There are various reasons for enabling another version of your software. Most often, you might have an existing application that depends on a combination of modules, and some don't work with updated versions of Perl. Not all modules are compatible with other modules.&lt;/p&gt; &lt;p&gt;For the remainder of this article, we will look at how to install Perl modules using streams.&lt;/p&gt; &lt;h2&gt;Exploring streams in RHEL 8&lt;/h2&gt; &lt;p&gt;Find out what Perl modules are available using the &lt;code&gt;yum module list&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;# yum module list Last metadata expiration check: 1:45:10 ago on Tue 07 May 2019 04:18:01 PM CEST. […] Name Stream Profiles Summary […] parfait 0.5 common Parfait Module perl 5.24 common [d], Practical Extraction and Report Languag minimal e perl 5.26 [d] common [d], Practical Extraction and Report Languag minimal e perl-App-cpanminus 1.7044 [d] common [d] Get, unpack, build and install CPAN mod ules perl-DBD-MySQL 4.046 [d] common [d] A MySQL interface for Perl perl-DBD-Pg 3.7 [d] common [d] A PostgreSQL interface for Perl perl-DBD-SQLite 1.58 [d] common [d] SQLite DBI driver perl-DBI 1.641 [d] common [d] A database access API for Perl perl-FCGI 0.78 [d] common [d] FastCGI Perl bindings perl-YAML 1.24 [d] common [d] Perl parser for YAML php 7.2 [d] common [d], PHP scripting language devel, minim al […] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output shows that a module is available for both Perl 5.24 and Perl 5.26. These are called &lt;em&gt;streams&lt;/em&gt; in the modularity world, and they denote independent variants of the same software stack, usually different versions. The &lt;code&gt;[d]&lt;/code&gt; flag marks a default stream, which is the one installed if you do not explicitly enable a different stream. So the previous output explains why &lt;code&gt;yum&lt;/code&gt; installed Perl 5.26.3 and not one of the 5.24 micro versions.&lt;/p&gt; &lt;p&gt;In general, any module can have multiple streams. At most, one stream can be the default, and another stream can be enabled. An enabled stream takes precedence over a default one. If there is no enabled or a default stream, the content of the module is unavailable.&lt;/p&gt; &lt;p&gt;Now, let's make a few assumptions:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;You have an old application that you are migrating from RHEL 7.&lt;/li&gt; &lt;li&gt;The application depends on the &lt;code&gt;rh-perl524&lt;/code&gt; &lt;a href="https://www.redhat.com/en/resources/red-hat-software-collections"&gt;Red Hat Software Collections&lt;/a&gt; environment.&lt;/li&gt; &lt;li&gt;You want to give it a try on RHEL 8.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Based on these assumptions, you will want to install Perl 5.24 on RHEL 8.&lt;/p&gt; &lt;h2&gt;Enabling a stream&lt;/h2&gt; &lt;p&gt;First, switch the Perl module to the 5.24 stream:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;# yum module enable perl:5.24 Last metadata expiration check: 2:03:16 ago on Tue 07 May 2019 04:18:01 PM CEST. Problems in request: Modular dependency problems with Defaults: Problem 1: conflicting requests - module freeradius:3.0:8000020190425181943:75ec4169-0.x86_64 requires module(perl:5.26), but none of the providers can be installed - module perl:5.26:820181219174508:9edba152-0.x86_64 conflicts with module(perl:5.24) provided by perl:5.24:820190207164249:ee766497-0.x86_64 - module perl:5.24:820190207164249:ee766497-0.x86_64 conflicts with module(perl:5.26) provided by perl:5.26:820181219174508:9edba152-0.x86_64 Problem 2: conflicting requests - module freeradius:3.0:820190131191847:fbe42456-0.x86_64 requires module(perl:5.26), but none of the providers can be installed - module perl:5.26:820181219174508:9edba152-0.x86_64 conflicts with module(perl:5.24) provided by perl:5.24:820190207164249:ee766497-0.x86_64 - module perl:5.24:820190207164249:ee766497-0.x86_64 conflicts with module(perl:5.26) provided by perl:5.26:820181219174508:9edba152-0.x86_64 Dependencies resolved. ========================================================================================== Package Arch Version Repository Size ========================================================================================== Enabling module streams: perl 5.24 Transaction Summary ========================================================================================== Is this ok [y/N]: y Complete! Switching module streams does not alter installed packages (see 'module enable' in dnf(8) for details)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We get a warning that the &lt;code&gt;freeradius:3.0&lt;/code&gt; stream is not compatible with &lt;code&gt;perl:5.24&lt;/code&gt;. That's because FreeRADIUS was built for Perl 5.26 only. Let's assume that your application fortunately doesn't require FreeRADIUS.&lt;/p&gt; &lt;p&gt;Next, the command displays a confirmation that it is enabling the Perl 5.24 stream. And, finally, there is another warning about installed packages. The last warning means that the system might still contain RPM packages from the Perl 5.26 stream, and you need to explicitly sort them out.&lt;/p&gt; &lt;p&gt;If you happen to receive the following error message instead, you have already enabled the &lt;code&gt;perl:5.26&lt;/code&gt; stream and &lt;code&gt;yum&lt;/code&gt; does not allow you to switch a module away from an already enabled stream for safety reasons:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;The operation would result in switching of module 'perl' stream '5.26' to stream '5.24' Error: It is not possible to switch enabled streams of a module unless explicitly enabled via configuration option module_stream_switch. It is recommended to rather remove all installed content from the module, and reset the module using 'yum module reset &lt;module_name&gt;' command. After you reset the module, you can install the other stream.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To recover from the safety block, follow the recommendation in the message and reset the &lt;code&gt;perl&lt;/code&gt; module with the &lt;code&gt;yum module reset perl&lt;/code&gt; command. Then you will be able to enable the &lt;code&gt;perl:5.24&lt;/code&gt; stream.&lt;/p&gt; &lt;p&gt;Changing modules and changing packages are two separate phases. You can fix it by synchronizing the distribution content like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;# yum --allowerasing distrosync Last metadata expiration check: 0:00:56 ago on Tue 07 May 2019 06:33:36 PM CEST. Modular dependency problems: Problem 1: module freeradius:3.0:8000020190425181943:75ec4169-0.x86_64 requires module(perl:5.26), but none of the providers can be installed - module perl:5.26:820181219174508:9edba152-0.x86_64 conflicts with module(perl:5.24) provided by perl:5.24:820190207164249:ee766497-0.x86_64 - module perl:5.24:820190207164249:ee766497-0.x86_64 conflicts with module(perl:5.26) provided by perl:5.26:820181219174508:9edba152-0.x86_64 - conflicting requests Problem 2: module freeradius:3.0:820190131191847:fbe42456-0.x86_64 requires module(perl:5.26), but none of the providers can be installed - module perl:5.26:820181219174508:9edba152-0.x86_64 conflicts with module(perl:5.24) provided by perl:5.24:820190207164249:ee766497-0.x86_64 - module perl:5.24:820190207164249:ee766497-0.x86_64 conflicts with module(perl:5.26) provided by perl:5.26:820181219174508:9edba152-0.x86_64 - conflicting requests Dependencies resolved. ========================================================================================== Package Arch Version Repository Size ========================================================================================== […] Downgrading: perl x86_64 4:5.24.4-403.module+el8+2770+c759b41a rhel-8.0.z-appstream 6.1 M […] Transaction Summary ========================================================================================== Upgrade 69 Packages Downgrade 66 Packages Total download size: 20 M Is this ok [y/N]: y […] Complete!&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Check your version with the &lt;code&gt;perl&lt;/code&gt; command again:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ perl -V:version version='5.24.4';&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Great! Installing the non-default module worked. The desired version of Perl is installed to a standard path (&lt;code&gt;/usr/bin/perl&lt;/code&gt;) and is therefore invoked with the &lt;code&gt;perl&lt;/code&gt; command. No &lt;code&gt;scl enable&lt;/code&gt; incantation is needed, in contrast to a requirement associated with the old software collections.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; A future &lt;code&gt;yum&lt;/code&gt; update will clean up the unnecessary warning about FreeRADIUS. I'll show some Perl-ish modules that are compatible with any Perl stream later in this article.&lt;/p&gt; &lt;h2&gt;Dependent modules&lt;/h2&gt; &lt;p&gt;Let's suppose that the old application mentioned earlier uses the &lt;code&gt;DBD::SQLite&lt;/code&gt; Perl CPAN module. So, let's install it. Yum can search for a CPAN module within a modularity module, so give the following a try:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;# yum --allowerasing install 'perl(DBD::SQLite)' […] Dependencies resolved. ========================================================================================== Package Arch Version Repository Size ========================================================================================== Installing: perl-DBD-SQLite x86_64 1.58-1.module+el8+2519+e351b2a7 rhel-8.0.z-appstream 186 k Installing dependencies: perl-DBI x86_64 1.641-2.module+el8+2701+78cee6b5 rhel-8.0.z-appstream 739 k Enabling module streams: perl-DBD-SQLite 1.58 perl-DBI 1.641 Transaction Summary ========================================================================================== Install 2 Packages Total download size: 924 k Installed size: 2.3 M Is this ok [y/N]: y […] Installed: perl-DBD-SQLite-1.58-1.module+el8+2519+e351b2a7.x86_64 perl-DBI-1.641-2.module+el8+2701+78cee6b5.x86_64 Complete!&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;DBD::SQLite&lt;/code&gt; CPAN module was found in the &lt;code&gt;perl-DBD-SQLite&lt;/code&gt; RPM package that's part of &lt;code&gt;perl-DBD-SQLite:1.58&lt;/code&gt; modularity module. It apparently requires some dependencies from the &lt;code&gt;perl-DBI:1.641&lt;/code&gt; modularity module too. After asking for confirmation, &lt;code&gt;yum&lt;/code&gt; enabled the necessary streams and installed the packages.&lt;/p&gt; &lt;p&gt;Before playing with &lt;code&gt;DBD::SQLite&lt;/code&gt; under Perl 5.24, take a look at the listing of the modularity modules and compare it with what we got the first time:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;# yum module list […] parfait 0.5 common Parfait Module perl 5.24 [e] common [d], Practical Extraction and Report Languag minimal e perl 5.26 [d] common [d], Practical Extraction and Report Languag minimal e perl-App-cpanminus 1.7044 [d] common [d] Get, unpack, build and install CPAN mod ules perl-DBD-MySQL 4.046 [d] common [d] A MySQL interface for Perl perl-DBD-Pg 3.7 [d] common [d] A PostgreSQL interface for Perl perl-DBD-SQLite 1.58 [d][e] common [d] SQLite DBI driver perl-DBI 1.641 [d][e] common [d] A database access API for Perl perl-FCGI 0.78 [d] common [d] FastCGI Perl bindings perl-YAML 1.24 [d] common [d] Perl parser for YAML php 7.2 [d] common [d], PHP scripting language devel, minim al […] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Notice that &lt;code&gt;perl:5.24&lt;/code&gt; is enabled (&lt;code&gt;[e]&lt;/code&gt;) and thus takes precedence over &lt;code&gt;perl:5.26&lt;/code&gt;, which would otherwise be a default stream (&lt;code&gt;[d]&lt;/code&gt;). Other enabled modularity modules are &lt;code&gt;perl-DBD-SQLite:1.58&lt;/code&gt; and &lt;code&gt;perl-DBI:1.641&lt;/code&gt;. Those were enabled when you installed &lt;code&gt;DBD::SQLite&lt;/code&gt;. These two modules have no other streams.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you need for some reason to disable a stream, even a default one, use the &lt;code&gt;yum module disable &lt;module&gt;:&lt;stream&gt;&lt;/code&gt; command.&lt;/p&gt; &lt;p&gt;Back to some productive work. You are ready to test the &lt;code&gt;DBD::SQLite&lt;/code&gt; CPAN module. Let's create a &lt;code&gt;test&lt;/code&gt; database containing a &lt;code&gt;foo&lt;/code&gt; table with one textual column called &lt;code&gt;bar&lt;/code&gt;, and store a row containing the string &lt;code&gt;Hello&lt;/code&gt; there:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ perl -MDBI -e '$dbh=DBI-&gt;connect(q{dbi:SQLite:dbname=test}); $dbh-&gt;do(q{CREATE TABLE foo (bar text)}); $sth=$dbh-&gt;prepare(q{INSERT INTO foo(bar) VALUES(?)}); $sth-&gt;execute(q{Hello})'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, verify that the &lt;code&gt;Hello&lt;/code&gt; string was indeed stored by querying the database:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ perl -MDBI -e '$dbh=DBI-&gt;connect(q{dbi:SQLite:dbname=test}); print $dbh-&gt;selectrow_array(q{SELECT bar FROM foo}), qq{\n}' Hello&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output shows that &lt;code&gt;DBD::SQLite&lt;/code&gt; works.&lt;/p&gt; &lt;h2&gt;Installing non-modular packages with non-default streams&lt;/h2&gt; &lt;p&gt;So far, everything we want is working. But now let's see what happens if you try to install incompatible RPM packages:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;# yum --allowerasing install 'perl(LWP)' […] Error: Problem: package perl-libwww-perl-6.34-1.el8.noarch requires perl(:MODULE_COMPAT_5.26.2), but none of the providers can be installed - cannot install the best candidate for the job - package perl-libs-4:5.26.3-416.el8.i686 is excluded - package perl-libs-4:5.26.3-416.el8.x86_64 is excluded (try to add '--skip-broken' to skip uninstallable packages or '--nobest' to use not only best candidate packages)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Yum reports an error about &lt;code&gt;perl-libwww-perl&lt;/code&gt; RPM package being incompatible. The &lt;code&gt;LWP&lt;/code&gt; CPAN module that is packaged as &lt;code&gt;perl-libwww-perl&lt;/code&gt; is built only for Perl 5.26, so RPM dependencies cannot be satisfied.&lt;/p&gt; &lt;p&gt;When a &lt;code&gt;perl:5.24&lt;/code&gt; stream is enabled, the packages from the &lt;code&gt;perl:5.26&lt;/code&gt; stream are masked, meaning that they become unavailable. However, this masking does not apply to non-modular packages, such as &lt;code&gt;perl-libwww-perl&lt;/code&gt;. Many packages have not been modularized yet. If you need some of them to be available and compatible with a non-default stream (i.e., not only with Perl 5.26) do not hesitate to contact the &lt;a href="https://access.redhat.com/support"&gt;Red Hat support team&lt;/a&gt; with your request. However, make sure that your desired non-default stream &lt;a href="https://access.redhat.com/support/policy/updates/rhel8-app-streams-life-cycle"&gt;is still supported&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Resetting a module&lt;/h2&gt; &lt;p&gt;Let's say you want to find out whether your existing application works with the new Perl 5.26 version. To do that, you need to switch back to the &lt;code&gt;perl:5.26&lt;/code&gt; stream.&lt;/p&gt; &lt;p&gt;Unfortunately, it's not a straightforward process to switch from an enabled stream back to a default stream or to yet another non-default stream. You'll need to perform a module reset:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;# yum module reset perl […] Dependencies resolved. ========================================================================================== Package Arch Version Repository Size ========================================================================================== Resetting module streams: perl 5.24 Transaction Summary ========================================================================================== Is this ok [y/N]: y Complete!&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That didn't hurt too much. Now you can synchronize the distribution again to replace the 5.24 RPM packages with 5.26 ones:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;# yum --allowerasing distrosync […] Transaction Summary ========================================================================================== Upgrade 65 Packages Downgrade 71 Packages Total download size: 22 M Is this ok [y/N]: y […]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After that, you can check the Perl version:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ perl -V:version version='5.26.3';&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And check the enabled modules:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;# yum module list […] parfait 0.5 common Parfait Module perl 5.24 common [d], Practical Extraction and Report Languag minimal e perl 5.26 [d] common [d], Practical Extraction and Report Languag minimal e perl-App-cpanminus 1.7044 [d] common [d] Get, unpack, build and install CPAN mod ules perl-DBD-MySQL 4.046 [d] common [d] A MySQL interface for Perl perl-DBD-Pg 3.7 [d] common [d] A PostgreSQL interface for Perl perl-DBD-SQLite 1.58 [d][e] common [d] SQLite DBI driver perl-DBI 1.641 [d][e] common [d] A database access API for Perl perl-FCGI 0.78 [d] common [d] FastCGI Perl bindings perl-YAML 1.24 [d] common [d] Perl parser for YAML php 7.2 [d] common [d], PHP scripting language devel, minim al […] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You are back at square one. The &lt;code&gt;perl:5.24&lt;/code&gt; stream is not enabled, and &lt;code&gt;perl:5.26&lt;/code&gt; is the default and therefore preferred. Only the &lt;code&gt;perl-DBD-SQLite:1.58&lt;/code&gt; and &lt;code&gt;perl-DBI:1.641&lt;/code&gt; streams remained enabled, which does not matter much because those are the only streams. Nonetheless, you can reset them back using &lt;code&gt;yum module reset perl-DBI perl-DBD-SQLite&lt;/code&gt; if you like.&lt;/p&gt; &lt;h2&gt;Multicontextual streams&lt;/h2&gt; &lt;p&gt;What happened with the &lt;code&gt;DBD::SQLite&lt;/code&gt; CPAN module? It's still there and working:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ perl -MDBI -e '$dbh=DBI-&gt;connect(q{dbi:SQLite:dbname=test}); print $dbh-&gt;selectrow_array(q{SELECT bar FROM foo}), qq{\n}' Hello&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command works because the &lt;code&gt;perl-DBD-SQLite&lt;/code&gt; module is built for both the 5.24 and 5.26 Perl versions. We call these modules &lt;em&gt;multicontextual&lt;/em&gt;. That's also the case for &lt;code&gt;perl-DBD-MySQL&lt;/code&gt; or &lt;code&gt;perl-DBI&lt;/code&gt;, but not the case for FreeRADIUS, which explains the warning you saw earlier. If you want to see these low-level details—such as which contexts are available, which dependencies are required, or which packages are contained in a module—you can use the &lt;code&gt;yum module info &lt;module&gt;:&lt;stream&gt;&lt;/code&gt; command.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;I hope this tutorial has shed some light on modules, a Red Hat Enterprise Linux 8 feature that lets you install multiple versions of software on top of one Linux platform. If you need more details, please read the &lt;a href="https://developers.redhat.com/rhel8/"&gt;documentation accompanying the product&lt;/a&gt; (namely, the userspace component management document and the &lt;a href="http://man7.org/linux/man-pages/man8/yum.8.html"&gt;yum(8) manual page&lt;/a&gt;) or ask the support team for help.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/03/10/modular-perl-red-hat-enterprise-linux-8" title="Modular Perl in Red Hat Enterprise Linux 8"&gt;Modular Perl in Red Hat Enterprise Linux 8&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Petr Pisar</dc:creator><dc:date>2022-03-10T07:00:00Z</dc:date></entry><entry><title type="html">Devoxx UK 2022 - Designing Your Best Architecture Diagrams (accepted)</title><link rel="alternate" href="http://www.schabell.org/2022/03/devoxxuk-2022-designing-best-architecture-diagrams-accepted.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2022/03/devoxxuk-2022-designing-best-architecture-diagrams-accepted.html</id><updated>2022-03-10T06:00:00Z</updated><content type="html">As mentioned previously, I've pushed a few talks to the  conference being held from 11-13 of May in London. The Devoxx series is well know and I've attended and spoken at multiple sessions for the Belgian version in the past. It's very developer centric with a lot of good depth and knowledgable attendees. The selection committee decided that the workshop I submitted would fit their developer tools and productivity theme nicely so I'll be sharing this great material with you in London. We've been hosting and customising this open source diagram tooling for over four years now and it's gained quite a bit of traction for those interested in designing their own architectural diagrams without the complexity of learning a grand tooling layout.  We can get you started in just 30 minutes with this session! Diagraming is one of the most important communication tools for sharing your project and architectural ideas to your colleagues and teams. In this workshop walkthrough, attendees are exposed to an open source tool we host online for designing architecture diagrams like an expert. Attendees are walked through the following in just 30 mins: * open and explore the tooling in your favourite web browser * explore the provided asset libraries for drag-and-drop designing * learn about the three types of diagrams that make up a good design * create your first simple logical diagram * create your first simple schematic diagram * create a detailed diagram * how to export and import diagrams and elements from a diagram This session is an introduction to the free online workshop available for attendees to jump right into after the session. Each of the individual labs in this workshop are stand alone, allowing the attendee to focus on anything of interest without having to work through the previous labs. If you're looking to become more proficient in sharing your ideas, architectures, and projects visually to wider audiences you can't underestimate the value of a good diagram. Join us to learn the tips and tricks that make a good diagram such a good communication vehicle and how our tooling eases your design tasks. Then head homewards with a free online workshop just waiting for you to explore! (Architecture - Tools-in-Action) Really looking forward to seeing you all again in person, so join us in May for Devoxx UK in London!</content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title type="html">Content Based Routing with Quarkus and Kogito</title><link rel="alternate" href="https://blog.kie.org/2022/03/content-based-routing-with-quarkus-and-kogito.html" /><author><name>Matteo Mortari</name></author><id>https://blog.kie.org/2022/03/content-based-routing-with-quarkus-and-kogito.html</id><updated>2022-03-09T13:22:20Z</updated><content type="html">This is a second iteration of a , where we implemented EIP patterns using just Drools and Apache Camel. In this post instead, I want to share with you how to implement a complete, end-to-end Content Based Routing solution using as a developer platform, including: , , , and Apache Kafka as our message broker. I will make use of a Managed Service offering for Kafka, which you can try for free yourself too, by using this link: . I will also make use of the Red Hat Developer OpenShift Sandbox to deploy the application; you can try for free yourself by using this link: . CONTENT BASED ROUTING OVERVIEW Here is the revised Enterprise Integration Pattern diagram of the flow, with the new components: EIP Diagram of the Content Based Routing application The application keep the focus on routing healthcare-related messages; for this demo example, messages are routed accordingly to the following decision table rules: Message Routing rules in a DMN decision table The table above describes the rules of message routing in terms of the (business) domain model: * the sending application * the type of message * the type of event For the purpose of this demo, the examples are provided using HL7v2 as the technical format for the message payload. You can read more about HL7v2 on the and on this . In order to properly translate from the specific technical format HL7v2 into the domain model, we can make use of the AtlasMap capabilities of data-mapping. This allows the stakeholder involved in the content based routing application to more easily inspect and describe the rules, for instance. Here is a visual summary of the AltasMap intent combined with the DMN decision table: Using AltasMap in combination with a DMN decision table In a about data enhancement, I hinted at combining the capabilities of AltasMap with DMN; I hope this tutorial now provides a very pragmatic example! TECHNICAL DETAILS In this section, I want to highlight how the allows to implement the EIP pattern very easily: from("direct:hl7") .enrich("direct:label", aggregationStrategy) .to("log:org.drools.demo?level=DEBUG&amp;amp;showAll=true&amp;amp;multiline=true") .routingSlip(header("whereTo")) .transform(HL7.ack()) ; from("direct:label") .unmarshal().hl7() .to("atlasmap:atlasmap-mapping.adm").unmarshal().json() .process(kogitoDMNEvaluate) // &lt;== Rules as DMN decisions .setHeader("topicsHeader", simple("${body[topic names]}")) ; As you can see, that’s all needed in order to implement the Enterprise Integration Pattern in a Quarkus application, and integrate it with AltasMap and Kogito. You can access the source code at this git repository: . DEPLOYMENT After setting up the Managed Kafka and OpenShift Sandbox accounts using the links provided above, the deployment is pretty straightforward. First, we create the intended Kafka topics on the Managed Kafka console. Creating the topic (queues) in the Managed Kafka Second, we deploy the content based routing Quarkus application using the OpenShift console. The content based routing application now deployed on OpenShift Don’t forget you can easily recreate the same setup yourself and for free, by using the links provided earlier in this post. For instance, I used the very same links myself in order to make sure the demo worked fine using free resources only. Finally, the deployment and setup is complete, and we can start to make use of our content based routing solution, by sending REST calls to the ingress endpoint; this can be used as a classic webhook or analogous to a . Invoking the REST API with an EDI message payload in HL7v2 format, and it is routed to the correct queue CONCLUSIONS To see a demonstration of this setup in action, don’t forget to check out the video linked at the beginning of this post! For example, the video shows the application responding live to the incoming messages in order to route them to the expected Kafka topic. Finally, I hope this article is helpful to you as a pragmatic example on how to implement a complete content based routing solution using Quarkus, Drools DMN and Apache Camel. Feedback? Questions? Don’t hesitate to let us know! The post appeared first on .</content><dc:creator>Matteo Mortari</dc:creator></entry><entry><title>Test GitHub projects with GitHub Actions and Testing Farm</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/03/09/test-github-projects-github-actions-and-testing-farm" /><author><name>Petr Hracek, Zuzana Miklankova</name></author><id>a7128059-ac74-4a72-90e3-95abcd91db53</id><updated>2022-03-09T07:00:00Z</updated><published>2022-03-09T07:00:00Z</published><summary type="html">&lt;p&gt;Every project on GitHub that's destined for &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL), Fedora, or CentOS should be tested before its changes are synced into a Git distribution repository (&lt;code&gt;dist-git&lt;/code&gt;). It's important to catch problems before delivering software to customers, and help quality assurance teams catch errors.&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.testing-farm.io/general/0.1/index.html"&gt;Testing Farm&lt;/a&gt; is an open source testing system offered as a service. Testing Farm’s idea is similar to &lt;a href="https://en.wikipedia.org/wiki/Compile_farm"&gt;Compile Farms&lt;/a&gt;, but with a focus on executing automated tests. Its mission is to provide a reliable and scalable service for executing automated tests from various users, such as Fedora CI, RHEL CI, Packit, and others. The entry point for our users is an &lt;a href="http://api.dev.testing-farm.io"&gt;HTTP-based API&lt;/a&gt;. Testing Farm scales across various infrastructures, including private and public clouds.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://developers.redhat.com/products/softwarecollections/overview"&gt;Red Hat Software Collections&lt;/a&gt; team has developed a way that you can use our Testing Farm in upstream GitHub repositories. Keep reading to learn how to use Testing Farm to improve your project with &lt;em&gt;internal&lt;/em&gt; tests and catch errors before products are delivered to customers.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: This article assumes you are familiar with using GitHub Actions.&lt;/p&gt; &lt;h2&gt;Get started with Testing Farm and GitHub Actions&lt;/h2&gt; &lt;p&gt;Testing Farm has an &lt;a href="https://testing-farm.gitlab.io/api/"&gt;HTTP-based API&lt;/a&gt; that you use to manage testing jobs. Connecting to the Testing Farm API requires two (on demand) artifacts:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;A Testing Farm API key, either for public or private clouds or both&lt;/li&gt; &lt;li&gt;A URL for Testing Farm requests&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;To get the API key and request URL, see the Testing Farm &lt;a href="https://docs.testing-farm.io/general/0.1/onboarding.html"&gt;onboarding document&lt;/a&gt;. Once you've acquired these artifacts, &lt;a href="https://docs.github.com/en/actions/security-guides/encrypted-secrets"&gt;save them to your GitHub repository's secrets&lt;/a&gt; so they can be used correctly within GitHub Actions.&lt;/p&gt; &lt;p&gt;The minimal steps for configuring a GitHub Action for running tests in Testing Farm are as follows:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Write a Test Management Tool/Flexible Metadata Format (TMT/FMF) &lt;a href="https://tmt.readthedocs.io/en/stable/spec/plans.html"&gt;testing plan&lt;/a&gt; for working with Testing Farm&lt;/li&gt; &lt;li&gt;Write a GitHub Action that: &lt;ul&gt;&lt;li&gt;Specifies when to run&lt;/li&gt; &lt;li&gt;Creates user request for Testing Farm&lt;/li&gt; &lt;li&gt;Gets the test results&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Writing TMT/FMF plans&lt;/h2&gt; &lt;p&gt;Testing Farm requires TMT/FMF plans for executing its tests. You will need to provide information about the provisioned machine and prepare that machine before running the tests.&lt;/p&gt; &lt;p&gt;The TMT plan will be uploaded to the Testing Farm machine during the process of the GitHub Action and then be executed directly from it. The plan will prepare a proper test environment and call the specific command for starting the tests. Here's an example of a TMT plan:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; summary: TMT/TFT plan for running tests on CentOS 7 description: | Run tests on CentOS-7 discover: how: shell tests: - name: Run tests on CentOS-7 framework: shell test: cd /tmp/$REPO_NAME &amp;&amp; &lt;call test suite&gt; duration: 3h prepare: how: shell script: | # TODO install packages needed for tests git clone $REPO_URL /tmp/$REPO_NAME cd /tmp/$REPO_NAME git fetch origin +refs/pull/*:refs/remotes/origin/pr/* git checkout origin/pr/$PR_NUMBER/head git submodule update --init execute: how: tmt&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Environment variables used in the TMT plan will be explicitly delivered to the Testing Farm later in the GitHub Action.&lt;/p&gt; &lt;p&gt;You can find more details on how to write TMT plans in &lt;a href="https://tmt.readthedocs.io/en/stable/spec/plans.html"&gt;the official documentation&lt;/a&gt;. The Software Collections team's &lt;a href="https://github.com/sclorg/sclorg-testing-farm"&gt;source file repository&lt;/a&gt; contains illustrative examples of TMT/FMF testing plans for &lt;a href="https://github.com/sclorg/sclorg-testing-farm/blob/main/plans/centos7.fmf"&gt;CentOS 7&lt;/a&gt;, &lt;a href="https://github.com/sclorg/sclorg-testing-farm/blob/main/plans/fedora.fmf"&gt;Fedora&lt;/a&gt;, and &lt;a href="https://github.com/sclorg/sclorg-testing-farm/blob/main/plans/c9s.fmf"&gt;CentOS Stream 9&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Write the GitHub Action&lt;/h2&gt; &lt;p&gt;As noted, the TMT plan executes tests in the Testing Farm environment. However, you still need a way to let Testing Farm know specifically when a test should be run. For this, you need to send an HTTP POST request to Testing Farm’s API; the request can be supplied directly from within the GitHub Action.&lt;/p&gt; &lt;p&gt;A GitHub Action used in this way maintains continuity of its execution with the user’s activity in the GitHub repository—a pull request, commit, or branch merging, for instance.&lt;/p&gt; &lt;h2&gt;Run tests on explicit user request&lt;/h2&gt; &lt;p&gt;This approach triggers tests at Testing Farm by commenting with specific text on a pull request. In this example, the text is the string &lt;code&gt;[test]&lt;/code&gt;. Only upstream project owners or members are authorized to trigger the tests:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; name: upstream tests at Testing Farm on: issue_comment: types: -created jobs: build: name: A job run on explicit user request run-ons: ubuntu-20.04 if: | github.event.issue.pull_request &amp;&amp; contains(github.event.comment.body, '[test]') &amp;&amp; contains(fromJson('["OWNER", "MEMBER"]'), github.event.comment.author_association) &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Triggering a test&lt;/h2&gt; &lt;p&gt;Testing Farm expects POST HTTP JSON requests as input. To handle this in a GitHub Action, install the &lt;code&gt;curl&lt;/code&gt; utility on the machine where the action will execute, along with the &lt;code&gt;jq&lt;/code&gt; utility for parsing JSON responses:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; - name: Schedule a test on Testing Farm id: sched_test run: | # Update ubuntu-20.04 in order to install curl and jq sudo apt update &amp;&amp; sudo apt -y install curl jq &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The JSON code in the request specifies the environment variables that you want to set in the Testing Farm environment. You also need to provide the &lt;code&gt;API_KEY&lt;/code&gt; and path to the TMT plan, as shown in the following listing. The request is then sent to a Testing Farm requests URL:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; - name: Schedule a test on Testing Farm id: sched_test run: | cat &lt;&lt; EOF &gt; request.json { "api_key": "${{ secrets.API_KEY }}", "test": {"fmf": { "url": "&lt;URL_TO_TMT_FMF_plan&gt;", "ref": "master", "name": "centos7" }}, "environments": [{ "arch": "x86_64", "os": {"compose": "CentOS-7"}, "variables": { "REPO_URL": "$GITHUB_SERVER_URL/$GITHUB_REPOSITORY", "REPO_NAME": "$GITHUB_REPOSITORY", "PR_NUMBER": "${{ github.event.comment.issue_url }}", "TEST_NAME": "test" } }] } EOF curl ${{ secrets.TF_ENDPOINT }}/requests --data @request.json --header "Content-Type: application/json" --output response.json &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The job ID will be embedded in the HTTP response from Testing Farm, which you can parse from the JSON and later use to query the results:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; # Store REQ_ID into outputs for later on usage req_id=$(jq -r .id response.json) echo "REQ_ID=$req_id" &gt;&gt; $GITHUB_ENV &lt;/code&gt; &lt;/pre&gt; &lt;h2&gt;Get the test results&lt;/h2&gt; &lt;p&gt;You can periodically request the state of your current test job from Testing Farm. The status request is an HTTP GET pointing to the Testing Farm URL endpoint—specifically, to &lt;code&gt;REQ_ID&lt;/code&gt;, which you stored in the previous step:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; curl ${{ secrets.TF_ENDPOINT }}/requests/${{ env.REQ_ID }} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When the job is completed, this information is stored in the value of the &lt;code&gt;state&lt;/code&gt; key. The final test result can be parsed from the &lt;code&gt;result&lt;/code&gt; key value.&lt;/p&gt; &lt;p&gt;Test logs can be gathered &lt;a href="http://artifacts.dev.testing-farm.io"&gt;within the job ID directory&lt;/a&gt; at testing-farm.io. Test results can be displayed as a status directly within a pull request with the &lt;a href="https://docs.github.com/en/rest/reference/repos#statuses"&gt;GitHub status API&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Run the test suite on Testing Farm&lt;/h2&gt; &lt;p&gt;At this point, you know how to set up everything you need to execute a test on Testing Farm infrastructure directly from a GitHub repository.&lt;/p&gt; &lt;p&gt;A full Testing Farm/GitHub Actions scheme example like the one described in this article can be found in the &lt;a href="https://github.com/sclorg/.github/blob/main/workflow-templates/docker-tests.yml"&gt; GitHub repository for the Software Collections team&lt;/a&gt;. This GitHub Action runs tests on Fedora, CentOS 7, CentOS Stream 9, RHEL 7, and RHEL 8. The action requires that the tested repository has access to the Testing Farm API and request URL, stored as GitHub secrets. The &lt;code&gt;GITHUB_TOKEN&lt;/code&gt; secret is necessary to connect to the GitHub API.&lt;/p&gt; &lt;p&gt;Testing Farm machines are situated within the Red Hat infrastructure, in case you are using Testing Farm private clouds. Therefore, all tests running on those machines must be checked for vulnerabilities and potentially dangerous behavior. Consequently, you should be sure to check the code in pull requests carefully from the role of owner or member.&lt;/p&gt; &lt;p&gt;Once you review the pull request, assuming you don't catch any problems or find any incorrect code, you can write a comment into the pull request containing the string &lt;code&gt;[test]&lt;/code&gt;. Submitting this string triggers the tests.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Important note:&lt;/strong&gt; This article has described how to set up the GitHub Action so that only the owner or a member of the GitHub organization can trigger the tests, a constraint we introduced for security reasons.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The combination of GitHub Actions and the Testing Farm environment provides a robust and easy-to-configure testing platform. By using GitHub Actions, you can automate responses to activity in a GitHub repository and straightforwardly display test results directly in the same repository.&lt;/p&gt; &lt;p&gt;Testing Farm has an infrastructure of machines available on request, incorporating RHEL, CentOS, and Fedora distributions with wide configuration options adjustable for any use case. Bringing those features together offers developers and testers a great opportunity to detect defects before they enter the product codebase.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/03/09/test-github-projects-github-actions-and-testing-farm" title="Test GitHub projects with GitHub Actions and Testing Farm"&gt;Test GitHub projects with GitHub Actions and Testing Farm&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Petr Hracek, Zuzana Miklankova</dc:creator><dc:date>2022-03-09T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus 2.7.4.Final released - Maintenance release</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-2-7-4-final-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-2-7-4-final-released/</id><updated>2022-03-09T00:00:00Z</updated><content type="html">We just released Quarkus 2.7.4.Final with a new round of bugfixes and documentation improvements. It is a safe upgrade for anyone already using 2.7. If you are not using 2.7 already, please refer to the 2.7 migration guide. Full changelog You can get the full changelog of 2.7.4.Final on GitHub....</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title>Inject custom JDK Flight Recorder events in containerized applications</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/03/08/inject-custom-jdk-flight-recorder-events-containerized-applications" /><author><name>Joshua Matsuoka</name></author><id>2f93526b-b0bf-4b62-8ec7-fd81a52c8f63</id><updated>2022-03-08T07:00:00Z</updated><published>2022-03-08T07:00:00Z</published><summary type="html">&lt;p&gt;The JDK Mission Control (JMC) agent is a powerful tool that allows users to inject custom JDK Flight Recorder (JFR) events at runtime without needing to restart the &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; virtual machine. Just as the JMC agent plugin simplifies the process of using the agent in a non-containerized environment, the Cryostat agent plugin does the same for &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerized&lt;/a&gt; environments.&lt;/p&gt; &lt;p&gt;JMC agent support is now merged into Cryostat, and Cryostat supports various API handlers for using the JMC agent in a containerized environment. This article introduces the Cryostat agent and its API handlers.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; To get up to speed on the technologies discussed in this article, check out &lt;a href="https://developers.redhat.com/articles/2021/11/16/jvm-performance-monitoring-jmc-agent"&gt;JVM performance monitoring with JMC agent&lt;/a&gt; and &lt;a href="https://developers.redhat.com/blog/2021/01/25/introduction-to-containerjfr-jdk-flight-recorder-for-containers"&gt;Introduction to Cryostat: JDK Flight Recorder for containers&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Prerequisites and setup&lt;/h2&gt; &lt;p&gt;Cryostat agent support is available from the Cryostat project's upstream repository. To get started, pull and build the most recent version:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; $ git clone https://github.com/cryostatio/cryostat $ cd cryostat $ mvn clean verify &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once it's been built, you can run a demo instance of Cryostat via the &lt;code&gt;smoketest&lt;/code&gt; script:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; $ smoketest.sh &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In order to use the JMC agent integration with Cryostat, you must ensure that its JAR is present in the same container as the application. Then, run the application with the &lt;code&gt;javaagent&lt;/code&gt; flag specifying the JAR:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; -javaagent:target/org.openjdk.jmc.agent-1.0.0-SNAPSHOT.jar &lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; You can download the JMC agent from &lt;a href="https://github.com/adoptium/jmc-overrides/releases"&gt;Adoptium's JMC overrides releases&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Using the Cryostat JMC agent integration&lt;/h2&gt; &lt;p&gt;The Cryostat agent offers a set of API handlers that interact with the JMC agent and manage templates for custom events. The &lt;code&gt;ProbeTemplate&lt;/code&gt; handlers allow users to post or delete probe templates. These templates are XML files that describe the custom events to be injected; they are stored with Cryostat and can be applied to any valid targets with the agent running, or deleted when they are no longer needed. Similarly, the &lt;code&gt;TargetProbePost&lt;/code&gt;, &lt;code&gt;Delete&lt;/code&gt;, and &lt;code&gt;Get&lt;/code&gt; handlers facilitate adding, removing, and retrieving custom event configurations from target JVMs.&lt;/p&gt; &lt;h3&gt;Custom event monitoring&lt;/h3&gt; &lt;p&gt;For an example workflow, suppose the Cryostat agent is currently running with Cryostat itself as the target application. Now, you want to add custom events to Cryostat to monitor how it's running. To start, you need a probe template to work with, so consider the following:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; &lt;jfragent&gt; &lt;!-- Global configuration options --&gt; &lt;config&gt; &lt;!-- This is the prefix to use when generating event class names. --&gt; &lt;classprefix&gt;__JFREvent&lt;/classprefix&gt; &lt;!-- Will allow the recording of arrays and object parameters as Strings. This will cause toString to be called for array elements and objects other than strings, which in turn can cause trouble if the toString method is badly implemented. Use with care. --&gt; &lt;allowtostring&gt;true&lt;/allowtostring&gt; &lt;!-- Allows converters to be used. See the org.openjdk.jmc.agent.converters package for more information. --&gt; &lt;allowconverter&gt;true&lt;/allowconverter&gt; &lt;/config&gt; &lt;events&gt; &lt;event id="demo.cryostat.jfr"&gt; &lt;label&gt;CryostatDemoEvent&lt;/label&gt; &lt;description&gt;Event for the agent plugin demo&lt;/description&gt; &lt;path&gt;demo&lt;/path&gt; &lt;stacktrace&gt;true&lt;/stacktrace&gt; &lt;class&gt;io.cryostat.net.web.http.generic.HealthGetHandler&lt;/class&gt; &lt;method&gt; &lt;name&gt;handle&lt;/name&gt; &lt;descriptor&gt;(io/vertx/ext/web/RoutingContext;)V&lt;/descriptor&gt; &lt;/method&gt; &lt;!-- location {ENTRY, EXIT, WRAP}--&gt; &lt;location&gt;ENTRY&lt;/location&gt; &lt;/event&gt; &lt;/events&gt; &lt;/jfragent&gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is a simple configuration that adds a custom event to the entry point of the &lt;code&gt;handle&lt;/code&gt; method. Once injected, this configuration will emit a custom &lt;code&gt;CryostatDemoEvent&lt;/code&gt; every time the &lt;code&gt;handle&lt;/code&gt; method is called.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;&lt;config&gt;&lt;/code&gt; section of the template determines the prefix used for the custom event classes, as well as whether any advanced functionality is required. The Cryostat agent supports the capture of fields and method parameters to be emitted with the event. It then utilizes user-defined converter methods to convert those fields and parameters into content types that JDK Flight Recorder can use. However, this functionality must be specifically enabled in the &lt;code&gt;&lt;config&gt;&lt;/code&gt; section.&lt;/p&gt; &lt;p&gt;Following the &lt;code&gt;&lt;config&gt;&lt;/code&gt; section, you can specify any number of events to be injected, you only need to provide an event ID, label, and description. The &lt;code&gt;&lt;path&gt;&lt;/code&gt; element corresponds to the path that the event will appear under when viewed in a graphical tool like JDK Mission Control. The &lt;code&gt;&lt;stacktrace&gt;&lt;/code&gt; element determines if stack traces should be recorded with the event.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;&lt;class&gt;&lt;/code&gt;, &lt;code&gt;&lt;method&gt;&lt;/code&gt;, and &lt;code&gt;&lt;location&gt;&lt;/code&gt; elements determine precisely where the event should be injected. You must provide the formal descriptor for the method here; this takes the form: &lt;code&gt;(Parameter Descriptor*)Return Descriptor&lt;/code&gt;. For example, for the method &lt;code&gt;int isAlive()&lt;/code&gt;, the method descriptor would be &lt;code&gt;()Z&lt;/code&gt;. See the &lt;a href="https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html#jvms-4.3.3"&gt;JVM specification&lt;/a&gt; for more about the method descriptor format.&lt;/p&gt; &lt;h3&gt;Injecting and recording custom events&lt;/h3&gt; &lt;p&gt;Once you have a probe template, all you need to do is send a few API requests to Cryostat to upload it and apply it to the target. (Remember that, in this case, the target is Cryostat itself.)&lt;/p&gt; &lt;pre&gt; &lt;code&gt; $ curl -F probeTemplate=@cryostat-probe.xml http://0.0.0.0:8181/api/v2/probes/ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This first request calls out to the &lt;code&gt;ProbeTemplateUpload&lt;/code&gt; handler (&lt;code&gt;api/v2/probes&lt;/code&gt;). This handler expects the &lt;code&gt;ProbeTemplate&lt;/code&gt; to be uploaded in a form with the name &lt;code&gt;probeTemplate&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Now you just have to send one more request:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; $ curl -X POST localhost:8181/api/v2/targets/localhost/probes/cryostat-probes &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The second request calls out to the &lt;code&gt;TargetProbePost&lt;/code&gt; handler (&lt;code&gt;api/v2/targets/:targetID/probes/:probeTemplate&lt;/code&gt;). When you provide this handler with a target and a probe template, it will apply the probe template to the target, injecting the custom events. The next time a recording is run, the custom events will be recorded.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The JMC agent is a powerful tool for creating and recording custom events without the need to manually write those events or rebuild an application. The Cryostat agent brings the same advantages to a containerized environment. With JMC agent support being merged upstream, it is now easier than ever to take advantage of these capabilities.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/03/08/inject-custom-jdk-flight-recorder-events-containerized-applications" title="Inject custom JDK Flight Recorder events in containerized applications"&gt;Inject custom JDK Flight Recorder events in containerized applications&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Joshua Matsuoka</dc:creator><dc:date>2022-03-08T07:00:00Z</dc:date></entry><entry><title type="html">Kogito 1.18.0 released!</title><link rel="alternate" href="https://blog.kie.org/2022/03/kogito-1-18-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2022/03/kogito-1-18-0-released.html</id><updated>2022-03-08T02:12:47Z</updated><content type="html">We are glad to announce that the Kogito 1.18.0 release is now available! This goes hand in hand with , release. From a feature point of view, we included a series of new features and bug fixes, including: * Spring Boot upgraded to version to 2.4.9 * Serverless Workflow data input schema validation * Serverless Workflow Rest Exception handler  * Serverless Workflow function reference replacement (for constraints and expressions) * Serverless Workflow magic word for retrieving process metadata ($WORKFLOW.instanceId) * Allow changing default incoming/outgoing channel name for messaging addon * Allow MessageDecorator override to set Kafka record key For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.17.0 artifacts are available at the . A detailed changelog for 1.18.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title type="html">Quarkus Tools for IntelliJ 1.10.0 released!</title><link rel="alternate" href="https://quarkus.io/blog/intellij-quarkus-tools-1.10.0/" /><author><name>Jeff Maury</name></author><id>https://quarkus.io/blog/intellij-quarkus-tools-1.10.0/</id><updated>2022-03-08T00:00:00Z</updated><content type="html">We are very pleased to announce the 1.10.0 release of Quarkus Tools for IntelliJ. This release adds support for Quarkus run/debug configurations and provides several fixes (including security fixes). Quarkus configurations It is possible to run or debug a Quarkus application from the IDE configurations. Select the Run → Edit...</content><dc:creator>Jeff Maury</dc:creator></entry><entry><title>Manage Python security with Thoth's cloud-based dependency resolver</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/03/07/manage-python-security-thoths-cloud-based-dependency-resolver" /><author><name>Fridolin Pokorny, Maya Costantini</name></author><id>ebf965f8-c22d-4a15-81db-4afda04fd40b</id><updated>2022-03-07T07:00:00Z</updated><published>2022-03-07T07:00:00Z</published><summary type="html">&lt;p&gt;Developers and &lt;a href="https://developers.redhat.com/topics/data-science"&gt;data scientists&lt;/a&gt; who want to build healthy and high-performance &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; applications often face challenges related to dependency management, including security risks introduced by the installation of dependencies. This article presents a quick introduction to managing Python dependencies with &lt;a href="http://thoth-station.ninja/"&gt;Project Thoth&lt;/a&gt;. The included video tutorial shows you how Thoth's cloud-based resolver finds problems in your Python dependencies and execution environment. Thoth's resolver is a drop-in replacement for other Python resolvers such as pip, Pipenv, or Poetry. Thoth's resolution process can also be used in &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerized environments&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Thoth security for Python applications&lt;/h2&gt; &lt;p&gt;Containerized environments offer a way to deploy applications to cluster orchestrators such as &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; and &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. The &lt;a href="https://developers.redhat.com/articles/2021/11/25/build-and-extend-containerized-applications-project-thoth"&gt;base container image used also provides software&lt;/a&gt; that can be shipped with the application. Figure 1 shows the hardware and software underlying a typical Python application.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/stack.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/stack.png?itok=dzBdBDR-" width="1154" height="697" alt="Various hardware, operating system, and Python library dependencies form the environment for an application." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. Various hardware, operating system, and Python library dependencies form the environment for an application. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Thoth can be used to discover and guide the security aspects of containerized environments through successful dependency resolution. The following video tutorial is an overview of how Thoth's cloud-based resolver resolves Python application dependencies.&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;h2&gt;Managing vulnerabilities with Thoth&lt;/h2&gt; &lt;p&gt;Once you have an idea of how Thoth works, you can get started using its resolver to manage your Python dependencies. Our &lt;a href="https://redhat-scholars.github.io/managing-vulnerabilities-with-thoth/managing-vulnerabilities-with-thoth/"&gt;Managing vulnerabilities with Thoth tutorial&lt;/a&gt; guides you through installing and setting up the environment for Thoth's command-line utility, &lt;a href="https://pypi.org/project/thamos/"&gt;Thamos&lt;/a&gt;. You can start by using pip to install the utility:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;pip install thamos&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once you've installed Thamos, you can &lt;a href="https://redhat-scholars.github.io/managing-vulnerabilities-with-thoth/managing-vulnerabilities-with-thoth/"&gt;follow the instructions in the tutorial&lt;/a&gt; to inspect an application present in the &lt;a href="https://github.com/thoth-station/cli-examples/"&gt;Thoth Station cli-examples&lt;/a&gt; repository. The tutorial also illustrates how to manage applications and application dependencies using the classic &lt;a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life"&gt;Game of Life application&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git clone https://github.com/thoth-station/cli-examples cd cli-examples thamos advise&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The tutorial also presents a variety of command outputs and shows how to detect security flaws in your Python application dependencies. The &lt;a href="https://www.youtube.com/watch?v=UCt37waSa6g"&gt;linked extended video&lt;/a&gt; can walk you through key Thoth resolver features.&lt;/p&gt; &lt;h2&gt;Developing Project Thoth&lt;/h2&gt; &lt;p&gt;Project Thoth started as a research project in the &lt;a href="http://aicoe.ai"&gt;Artificial Intelligence Center of Excellence&lt;/a&gt; (AICoE) group in 2018. Initially, the Thoth team consisted of two engineers, but it quickly expanded with new interns and hires. From 2018 until the time of this writing, the core repositories of Project Thoth accepted contributions from 49 engineers, approximately half of them external to the Thoth team. The number of repositories associated with the &lt;a href="https://github.com/thoth-station"&gt;thoth-station organization on GitHub&lt;/a&gt; has grown to more than 180 (60 of which are now archived).&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;a href="http://thoth-station.ninja/"&gt;Project Thoth&lt;/a&gt; is also known as AIDevSecOps because of its role as part of a &lt;a href="https://developers.redhat.com/topics/devsecops"&gt;DevSecOps&lt;/a&gt; strategy.&lt;/p&gt; &lt;p&gt;To support data aggregation, we've switched our main database twice, and during the whole development phase, the project has been deployed on seven OpenShift clusters. The system generated more than 1.9 TiB of data in these clusters, which were stored in &lt;a href="https://ceph.io/"&gt;Ceph&lt;/a&gt;. The production &lt;a href="https://www.postgresql.org/"&gt;PostgreSQL&lt;/a&gt; database keeps more than 27GiB of mostly Python dependency data, aggregated by &lt;a href="https://developers.redhat.com/blog/2021/04/26/continuous-learning-in-project-thoth-using-kafka-and-argo"&gt;background aggregation logic&lt;/a&gt; that uses &lt;a href="https://argoproj.github.io/argo-workflows/"&gt;Argo Workflows&lt;/a&gt; and &lt;a href="https://strimzi.io/"&gt;Strimzi&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://argo-cd.readthedocs.io"&gt;Argo CD&lt;/a&gt; helps guarantee &lt;a href="https://www.redhat.com/en/topics/devops/what-is-gitops"&gt;GitOps best practices&lt;/a&gt; and supports observability through &lt;a href="https://grafana.com/"&gt;Grafana&lt;/a&gt; and OpenShift metrics exposed by OpenShift itself. &lt;a href="https://tekton.dev/"&gt;Tekton&lt;/a&gt; and &lt;a href="https://github.com/aicoe/aicoe-ci"&gt;AICoE-CI&lt;/a&gt; help automate builds of container images that are &lt;a href="http://quay.io/organization/thoth-station"&gt;hosted on Quay&lt;/a&gt;. &lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow"&gt;Prow&lt;/a&gt; checks make sure that developers deliver high-quality contributions.&lt;/p&gt; &lt;p&gt;Engineers have given &lt;a href="https://github.com/thoth-station/talks"&gt;talks about various parts of the Thoth project more than 25 times&lt;/a&gt; in North America and Europe.&lt;/p&gt; &lt;p&gt;All the statistics were aggregated as of this writing and we believe the project will continue to expand. You can learn more about Project Thoth by reading the following articles on Red Hat Developer:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/02/24/inspecting-containerized-python-applications-cluster"&gt;Inspecting containerized Python applications in a cluster&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/01/17/how-self-host-python-package-index-using-pulp"&gt;How to self-host a Python package index using Pulp&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/01/14/extracting-dependencies-python-packages"&gt;Extracting dependencies from Python packages&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/01/05/extracting-information-python-source-code"&gt;Extracting information from Python source code&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/12/21/prevent-python-dependency-confusion-attacks-thoth"&gt;Prevent Python dependency confusion attacks with Thoth&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/25/build-and-extend-containerized-applications-project-thoth"&gt;Build and extend containerized applications with Project Thoth&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/17/customize-python-dependency-resolution-machine-learning"&gt;Customize Python dependency resolution with machine learning&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/04/generating-pseudorandom-numbers-python"&gt;Generating pseudorandom numbers in Python&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/09/29/secure-your-python-applications-thoth-recommendations"&gt;Secure your Python applications with Thoth recommendations&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/10/06/find-and-compare-python-libraries-project2vec"&gt;Find and compare Python libraries with project2vec&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/09/22/thoth-prescriptions-resolving-python-dependencies"&gt;Thoth prescriptions for resolving Python dependencies&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/09/17/resolve-python-dependencies-thoth-dependency-monkey"&gt;Resolve Python dependencies with Thoth Dependency Monkey&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/05/19/micropipenv-installing-python-dependencies-containerized-applications"&gt;micropipenv: Installing Python dependencies in containerized applications&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2021/04/26/continuous-learning-in-project-thoth-using-kafka-and-argo"&gt;Continuous learning in Project Thoth using Kafka and Argo&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/05/26/can-we-consider-editable-bad-practice"&gt;Can we consider --editable a bad practice?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2021/03/19/managing-python-dependencies-with-the-thoth-jupyterlab-extension"&gt;Managing Python dependencies with the Thoth JupyterLab extension&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2020/12/24/use-kebechet-machine-learning-to-perform-source-code-operations"&gt;Use Kebechet machine learning to perform source code operations&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2020/09/30/ai-software-stack-inspection-with-thoth-and-tensorflow"&gt;AI software stack inspection with Thoth and TensorFlow&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2019/10/28/microbenchmarks-for-ai-applications-using-red-hat-openshift-on-psi-in-project-thoth"&gt;Microbenchmarks for AI applications using Red Hat OpenShift on PSI in project Thoth&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Connect with the Thoth team!&lt;/h2&gt; &lt;p&gt;As part of Project Thoth, we are accumulating knowledge to help Python developers create healthy applications. If you would like to follow updates, feel free to subscribe to our &lt;a href="https://www.youtube.com/channel/UClUIDuq_hQ6vlzmqM59B2Lw"&gt;YouTube channel&lt;/a&gt; or follow us on the &lt;a href="https://twitter.com/thothstation"&gt;@ThothStation&lt;/a&gt; Twitter handle.&lt;/p&gt; &lt;p&gt;Even though the project is in its early stages, we are constantly improving its stability and reliability. We would be happy for any feedback. To send us feedback or get involved in improving the Python ecosystem, please contact the Thoth Station &lt;a href="https://github.com/thoth-station/support"&gt;support repository&lt;/a&gt;. You can also directly reach out to the &lt;a href="https://twitter.com/ThothStation"&gt;Thoth team on Twitter&lt;/a&gt;. You can report any issues you've spotted in open source Python libraries to the support repository or &lt;a href="https://developers.redhat.com/articles/2021/09/22/thoth-prescriptions-resolving-python-dependencies"&gt;directly write prescriptions for the resolver&lt;/a&gt; and send them to our &lt;a href="https://github.com/thoth-station/prescriptions/"&gt;prescriptions repository&lt;/a&gt;. By participating in these ways, you can help the Python cloud-based resolver come up with better recommendations for the whole Python community.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/03/07/manage-python-security-thoths-cloud-based-dependency-resolver" title="Manage Python security with Thoth's cloud-based dependency resolver"&gt;Manage Python security with Thoth's cloud-based dependency resolver&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Fridolin Pokorny, Maya Costantini</dc:creator><dc:date>2022-03-07T07:00:00Z</dc:date></entry><entry><title type="html">DevOpsDays Raleigh 2022 - Talking Architecture Shop (accepted)</title><link rel="alternate" href="http://www.schabell.org/2022/03/devopsdays-raleigh-2022-talking-architecture-shop-accepted.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2022/03/devopsdays-raleigh-2022-talking-architecture-shop-accepted.html</id><updated>2022-03-07T06:00:00Z</updated><content type="html"> I've that I was trying to join the DevOpsDays Raleigh 2022 conference this year, to be hosted on April 13-14 in Raleigh, NC.  I submitted a few talks, a workshop, and crossed my fingers to await the selection committee results. The verdict is in, I've got a session talking architecture shop selected. It's going to be fun to share my teams research and development of open source DevOps architectures. Here's the talk and abstract and really looking forward to seeing you all in person! My session is from a series called Talking Architecture Shop. This is focusing on architecture research for solutions in the DevOps domain that scale and will be co-presented with my good friend . TALKING ARCHITECTURE SHOP - EXPLORING OPEN SOURCE DEVOPS AT SCALE  You've heard of large scale open source architectures, but have you ever wanted to take a serious look at these real life enterprise DevOps implementations that scale? This session takes attendees on a tour of multiple use cases covering DevOps challenges with hybrid cloud management with GitOps, DevOps in healthcare, and much more. Not only are these architectures interesting, but they are successful real life implementations featuring open source technologies and power many of your own online experiences. The attendee departs this session with a working knowledge of how to map general open source technologies to their solutions. Material covered is available freely online and attendees can use these solutions as starting points for aligning to their own solution architectures. Join us for an hour of power as we talk architecture shop!  Finally, remember to  and join the fun for a really interesting array of DevOps talks and workshops. </content><dc:creator>Eric D. Schabell</dc:creator></entry></feed>
